{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MABe Mouse Behavior - Optimized Complete Solution\n",
    "\n",
    "## Full Pipeline:\n",
    "```\n",
    "Phase 1: Lightweight Transformer\n",
    "  ├─ Load top 120 labeled videos\n",
    "  ├─ Extract sequences (aggressive subsampling)\n",
    "  ├─ Train tiny transformer (5 epochs)\n",
    "  └─ Save checkpoint\n",
    "\n",
    "Phase 2: Feature Extraction\n",
    "  ├─ Extract transformer embeddings\n",
    "  ├─ Compute essential handcrafted features\n",
    "  └─ Cache combined features\n",
    "\n",
    "Phase 3: XGBoost Training\n",
    "  ├─ Load features\n",
    "  ├─ Train per action\n",
    "  └─ Fast threshold optimization\n",
    "\n",
    "Phase 4: Test Inference\n",
    "  ├─ Extract test features\n",
    "  ├─ Predict\n",
    "  └─ Create submission\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json, gc, warnings, itertools, pickle, os, time\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import f1_score\n",
    "from xgboost import XGBClassifier\n",
    "import optuna\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('/kaggle/input'):\n",
    "    input_dirs = [str(d) for d in Path('/kaggle/input').iterdir() if d.is_dir()]\n",
    "    DATA_DIR = next((d for d in input_dirs if 'mabe' in d.lower()), \"/kaggle/input/mabe-mouse-behavior-detection\")\n",
    "else:\n",
    "    DATA_DIR = \"data/mabe-mouse-behavior-detection\"\n",
    "\n",
    "print(f\"Data: {DATA_DIR}\")\n",
    "\n",
    "class Config:\n",
    "    TRAIN_PATH = f\"{DATA_DIR}/train.csv\"\n",
    "    TEST_PATH = f\"{DATA_DIR}/test.csv\"\n",
    "    TRAIN_ANNOTATION_DIR = f\"{DATA_DIR}/train_annotation\"\n",
    "    TRAIN_TRACKING_DIR = f\"{DATA_DIR}/train_tracking\"\n",
    "    TEST_TRACKING_DIR = f\"{DATA_DIR}/test_tracking\"\n",
    "    \n",
    "    # Checkpoints\n",
    "    TRANSFORMER_PATH = \"transformer.pth\"\n",
    "    FEATURES_PATH = \"features.pkl\"\n",
    "    \n",
    "    # Phase 1: Transformer\n",
    "    SEQ_LEN = 30\n",
    "    EMBED_DIM = 64\n",
    "    N_HEADS = 4\n",
    "    N_LAYERS = 2\n",
    "    DROPOUT = 0.1\n",
    "    TRANSFORMER_EPOCHS = 8  # Increased from 5 for better training\n",
    "    BATCH_SIZE = 256\n",
    "    LR = 1e-3\n",
    "    MAX_TRAIN_VIDEOS = 120  # Subset for transformer\n",
    "    \n",
    "    # Feature engineering\n",
    "    FPS_DEFAULT = 30.0  # Default frames per second\n",
    "    \n",
    "    # Phase 2: Feature extraction\n",
    "    MAX_FRAMES = 6000  # Per video\n",
    "    STRIDE = 2\n",
    "    KEY_PARTS = ['nose', 'body_center', 'tail_base', 'ear_left', 'ear_right']\n",
    "    N_FEATURES = len(KEY_PARTS) * 2\n",
    "    \n",
    "    # Phase 3: XGBoost\n",
    "    N_SPLITS = 2\n",
    "    XGB_PARAMS = {\n",
    "        'n_estimators': 150,\n",
    "        'learning_rate': 0.1,\n",
    "        'max_depth': 5,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.7,\n",
    "        'tree_method': 'hist',\n",
    "        'random_state': 42,\n",
    "        'n_jobs': 4,\n",
    "        'verbosity': 0\n",
    "    }\n",
    "    \n",
    "    DROP_PARTS = ['headpiece_bottombackleft', 'headpiece_bottombackright',\n",
    "                  'headpiece_bottomfrontleft', 'headpiece_bottomfrontright',\n",
    "                  'headpiece_topbackleft', 'headpiece_topbackright',\n",
    "                  'headpiece_topfrontleft', 'headpiece_topfrontright',\n",
    "                  'spine_1', 'spine_2', 'tail_middle_1', 'tail_middle_2', 'tail_midpoint']\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(cfg.TRAIN_PATH)\n",
    "test = pd.read_csv(cfg.TEST_PATH)\n",
    "train_labeled = train[~train['lab_id'].str.startswith('MABe22')].reset_index(drop=True)\n",
    "\n",
    "print(f\"Labeled: {len(train_labeled)}, Test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1: Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Series Transformer (based on \"A Transformer-based Framework for Multivariate Time Series\")\n",
    "# Reference: Zerveas et al. 2020 - https://arxiv.org/abs/2010.02803\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, d_model]\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TSTransformerEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Time Series Transformer Encoder for Classification\n",
    "    Based on the paper: \"A Transformer-based Framework for Multivariate Time Series Representation Learning\"\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, d_model=128, n_heads=8, n_layers=3, \n",
    "                 dim_feedforward=256, dropout=0.1, max_len=1000):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.project_inp = nn.Linear(n_features, d_model)\n",
    "        \n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len=max_len, dropout=dropout)\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True  # Pre-LN for better training stability\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "        \n",
    "        self.output_layer = nn.Linear(d_model, 1)\n",
    "        \n",
    "        self.gap = nn.AdaptiveAvgPool1d(1)\n",
    "        self.gmp = nn.AdaptiveMaxPool1d(1)\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    def forward(self, x, return_embedding=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [batch_size, seq_len, n_features]\n",
    "            return_embedding: if True, return embeddings instead of logits\n",
    "        Returns:\n",
    "            If return_embedding: [batch_size, d_model * 2] (concat of avg and max pooling)\n",
    "            Else: [batch_size] logits\n",
    "        \"\"\"\n",
    "        x = self.project_inp(x)  # [batch, seq_len, d_model]\n",
    "        \n",
    "        x = self.pos_enc(x)\n",
    "        \n",
    "        x = self.transformer_encoder(x)  # [batch, seq_len, d_model]\n",
    "        \n",
    "        if return_embedding:\n",
    "            # Transpose for pooling: [batch, d_model, seq_len]\n",
    "            x_t = x.transpose(1, 2)\n",
    "            avg_pool = self.gap(x_t).squeeze(-1)  # [batch, d_model]\n",
    "            max_pool = self.gmp(x_t).squeeze(-1)  # [batch, d_model]\n",
    "            return torch.cat([avg_pool, max_pool], dim=1)  # [batch, d_model * 2]\n",
    "        else:\n",
    "            mid_idx = x.size(1) // 2\n",
    "            x_mid = x[:, mid_idx, :]  # [batch, d_model]\n",
    "            return self.output_layer(x_mid).squeeze(-1)  # [batch]\n",
    "\n",
    "model = TSTransformerEncoder(\n",
    "    n_features=cfg.N_FEATURES,\n",
    "    d_model=128,\n",
    "    n_heads=8,\n",
    "    n_layers=3,\n",
    "    dim_feedforward=512,\n",
    "    dropout=0.1,\n",
    "    max_len=cfg.SEQ_LEN * 2\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model: TSTransformerEncoder\")\n",
    "print(f\"Params: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pose(data, parts):\n",
    "    feat = []\n",
    "    for p in parts:\n",
    "        if p in data.columns:\n",
    "            feat.extend([data[p]['x'].values, data[p]['y'].values])\n",
    "        else:\n",
    "            feat.extend([np.zeros(len(data)), np.zeros(len(data))])\n",
    "    arr = np.stack(feat, 1).astype(np.float32)\n",
    "    \n",
    "    arr = np.nan_to_num(arr, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    mean = arr.mean(axis=0, keepdims=True)\n",
    "    std = arr.std(axis=0, keepdims=True) + 1e-6\n",
    "    arr = (arr - mean) / std\n",
    "    \n",
    "    return arr\n",
    "\n",
    "def make_seqs(feat, lab, slen, stride):\n",
    "    seqs, tgts = [], []\n",
    "    for i in range(0, len(feat)-slen, stride):\n",
    "        seqs.append(feat[i:i+slen])\n",
    "        tgts.append(lab[i+slen//2])\n",
    "    return np.array(seqs), np.array(tgts)\n",
    "\n",
    "class SeqData(Dataset):\n",
    "    def __init__(self, seqs, labs):\n",
    "        self.seqs = torch.FloatTensor(seqs)\n",
    "        self.labs = torch.FloatTensor(labs)\n",
    "    def __len__(self):\n",
    "        return len(self.seqs)\n",
    "    def __getitem__(self, i):\n",
    "        return self.seqs[i], self.labs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FPS-AWARE FEATURE ENGINEERING UTILITIES\n",
    "\n",
    "def _scale(n_frames_at_30fps, fps, ref=30.0):\n",
    "    \"\"\"Scale window size based on FPS\"\"\"\n",
    "    return max(1, int(round(n_frames_at_30fps * float(fps) / ref)))\n",
    "\n",
    "def _scale_signed(n_frames_at_30fps, fps, ref=30.0):\n",
    "    \"\"\"Scale with sign preservation for offsets\"\"\"\n",
    "    if n_frames_at_30fps == 0:\n",
    "        return 0\n",
    "    s = 1 if n_frames_at_30fps > 0 else -1\n",
    "    mag = max(1, int(round(abs(n_frames_at_30fps) * float(fps) / ref)))\n",
    "    return s * mag\n",
    "\n",
    "def add_curvature_features(X, center_x, center_y, fps):\n",
    "    \"\"\"Add curvature and turn rate features\"\"\"\n",
    "    vel_x = center_x.diff()\n",
    "    vel_y = center_y.diff()\n",
    "    acc_x = vel_x.diff()\n",
    "    acc_y = vel_y.diff()\n",
    "\n",
    "    cross_prod = vel_x * acc_y - vel_y * acc_x\n",
    "    vel_mag = np.sqrt(vel_x**2 + vel_y**2)\n",
    "    curvature = np.abs(cross_prod) / (vel_mag**3 + 1e-6)\n",
    "\n",
    "    for w in [25, 50, 75]:\n",
    "        ws = _scale(w, fps)\n",
    "        X[f'curv_mean_{w}'] = curvature.rolling(ws, min_periods=max(1, ws // 5)).mean()\n",
    "\n",
    "    angle = np.arctan2(vel_y, vel_x)\n",
    "    angle_change = np.abs(angle.diff())\n",
    "    w = 30\n",
    "    ws = _scale(w, fps)\n",
    "    X[f'turn_rate_{w}'] = angle_change.rolling(ws, min_periods=max(1, ws // 5)).sum()\n",
    "\n",
    "    return X\n",
    "\n",
    "def add_multiscale_features(X, center_x, center_y, fps):\n",
    "    \"\"\"Add multiscale speed analysis\"\"\"\n",
    "    speed = np.sqrt(center_x.diff()**2 + center_y.diff()**2) * float(fps)\n",
    "\n",
    "    scales = [20, 40, 60, 80]\n",
    "    for scale in scales:\n",
    "        ws = _scale(scale, fps)\n",
    "        if len(speed) >= ws:\n",
    "            X[f'sp_m{scale}'] = speed.rolling(ws, min_periods=max(1, ws // 4)).mean()\n",
    "            X[f'sp_s{scale}'] = speed.rolling(ws, min_periods=max(1, ws // 4)).std()\n",
    "\n",
    "    if len(scales) >= 2 and f'sp_m{scales[0]}' in X.columns and f'sp_m{scales[-1]}' in X.columns:\n",
    "        X['sp_ratio'] = X[f'sp_m{scales[0]}'] / (X[f'sp_m{scales[-1]}'] + 1e-6)\n",
    "\n",
    "    return X\n",
    "\n",
    "def add_state_features(X, center_x, center_y, fps):\n",
    "    \"\"\"Add state-based features (speed binning)\"\"\"\n",
    "    speed = np.sqrt(center_x.diff()**2 + center_y.diff()**2) * float(fps)\n",
    "    w_ma = _scale(15, fps)\n",
    "    speed_ma = speed.rolling(w_ma, min_periods=max(1, w_ma // 3)).mean()\n",
    "\n",
    "    try:\n",
    "        bins = [-np.inf, 0.5 * fps, 2.0 * fps, 5.0 * fps, np.inf]\n",
    "        speed_states = pd.cut(speed_ma, bins=bins, labels=[0, 1, 2, 3]).astype(float)\n",
    "\n",
    "        for window in [20, 40, 60, 80]:\n",
    "            ws = _scale(window, fps)\n",
    "            if len(speed_states) >= ws:\n",
    "                for state in [0, 1, 2, 3]:\n",
    "                    X[f's{state}_{window}'] = (\n",
    "                        (speed_states == state).astype(float)\n",
    "                        .rolling(ws, min_periods=max(1, ws // 5)).mean()\n",
    "                    )\n",
    "                state_changes = (speed_states != speed_states.shift(1)).astype(float)\n",
    "                X[f'trans_{window}'] = state_changes.rolling(ws, min_periods=max(1, ws // 5)).sum()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return X\n",
    "\n",
    "def add_longrange_features(X, center_x, center_y, fps):\n",
    "    \"\"\"Add long-range temporal features\"\"\"\n",
    "    for window in [30, 60, 120]:\n",
    "        ws = _scale(window, fps)\n",
    "        if len(center_x) >= ws:\n",
    "            X[f'x_ml{window}'] = center_x.rolling(ws, min_periods=max(5, ws // 6)).mean()\n",
    "            X[f'y_ml{window}'] = center_y.rolling(ws, min_periods=max(5, ws // 6)).mean()\n",
    "\n",
    "    for span in [30, 60, 120]:\n",
    "        s = _scale(span, fps)\n",
    "        X[f'x_e{span}'] = center_x.ewm(span=s, min_periods=1).mean()\n",
    "        X[f'y_e{span}'] = center_y.ewm(span=s, min_periods=1).mean()\n",
    "\n",
    "    speed = np.sqrt(center_x.diff()**2 + center_y.diff()**2) * float(fps)\n",
    "    for window in [30, 60, 120]:\n",
    "        ws = _scale(window, fps)\n",
    "        if len(speed) >= ws:\n",
    "            X[f'sp_pct{window}'] = speed.rolling(ws, min_periods=max(5, ws // 6)).rank(pct=True)\n",
    "\n",
    "    return X\n",
    "\n",
    "def add_interaction_features(X, mouse_pair, avail_A, avail_B, fps):\n",
    "    \"\"\"Add pair interaction features\"\"\"\n",
    "    if 'body_center' not in avail_A or 'body_center' not in avail_B:\n",
    "        return X\n",
    "\n",
    "    rel_x = mouse_pair['A']['body_center']['x'] - mouse_pair['B']['body_center']['x']\n",
    "    rel_y = mouse_pair['A']['body_center']['y'] - mouse_pair['B']['body_center']['y']\n",
    "    rel_dist = np.sqrt(rel_x**2 + rel_y**2)\n",
    "\n",
    "    A_vx = mouse_pair['A']['body_center']['x'].diff()\n",
    "    A_vy = mouse_pair['A']['body_center']['y'].diff()\n",
    "    B_vx = mouse_pair['B']['body_center']['x'].diff()\n",
    "    B_vy = mouse_pair['B']['body_center']['y'].diff()\n",
    "\n",
    "    A_lead = (A_vx * rel_x + A_vy * rel_y) / (np.sqrt(A_vx**2 + A_vy**2) * rel_dist + 1e-6)\n",
    "    B_lead = (B_vx * (-rel_x) + B_vy * (-rel_y)) / (np.sqrt(B_vx**2 + B_vy**2) * rel_dist + 1e-6)\n",
    "\n",
    "    for window in [30, 60]:\n",
    "        ws = _scale(window, fps)\n",
    "        X[f'A_ld{window}'] = A_lead.rolling(ws, min_periods=max(1, ws // 6)).mean()\n",
    "        X[f'B_ld{window}'] = B_lead.rolling(ws, min_periods=max(1, ws // 6)).mean()\n",
    "\n",
    "    approach = -rel_dist.diff()\n",
    "    chase = approach * B_lead\n",
    "    w = 30\n",
    "    ws = _scale(w, fps)\n",
    "    X[f'chase_{w}'] = chase.rolling(ws, min_periods=max(1, ws // 6)).mean()\n",
    "\n",
    "    for window in [60, 120]:\n",
    "        ws = _scale(window, fps)\n",
    "        A_sp = np.sqrt(A_vx**2 + A_vy**2)\n",
    "        B_sp = np.sqrt(B_vx**2 + B_vy**2)\n",
    "        X[f'sp_cor{window}'] = A_sp.rolling(ws, min_periods=max(1, ws // 6)).corr(B_sp)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENHANCED FEATURE EXTRACTION FUNCTIONS\n",
    "\n",
    "def enhanced_hand_feat(data, fps=30.0):\n",
    "    \"\"\"\n",
    "    Extract comprehensive handcrafted features with FPS awareness\n",
    "    \n",
    "    IMPORTANT: Always produces the same number of features regardless of missing body parts.\n",
    "    Missing parts are filled with zeros to ensure consistent feature dimensions.\n",
    "    \n",
    "    Features: FIXED dimension (~120-150 features)\n",
    "    - Basic speed (15, 30 frame windows)\n",
    "    - Rolling statistics (5, 15, 30, 60 frame windows)\n",
    "    - Distance pairs between all body parts\n",
    "    - Body shape (elongation, body angle)\n",
    "    - Speed with lags\n",
    "    - Curvature and turn rate\n",
    "    - Multiscale speed analysis\n",
    "    - State-based features\n",
    "    - Long-range temporal features\n",
    "    - Nose-tail features\n",
    "    - Ear features\n",
    "    \"\"\"\n",
    "    n_frames = len(data)\n",
    "    X = pd.DataFrame(index=data.index)\n",
    "    available_parts = data.columns.get_level_values(0).unique()\n",
    "    \n",
    "    # Helper to get part data or zeros\n",
    "    def get_part(part_name):\n",
    "        if part_name in available_parts:\n",
    "            return data[part_name]['x'], data[part_name]['y']\n",
    "        else:\n",
    "            return pd.Series(0.0, index=data.index), pd.Series(0.0, index=data.index)\n",
    "\n",
    "    # Body center features (ALWAYS added, zeros if missing)\n",
    "    cx, cy = get_part('body_center')\n",
    "    \n",
    "    # Basic speed features\n",
    "    speed = np.sqrt(cx.diff()**2 + cy.diff()**2) * fps\n",
    "    for w in [15, 30]:\n",
    "        ws = _scale(w, fps)\n",
    "        X[f's{w}'] = speed.rolling(ws, min_periods=1, center=True).mean()\n",
    "\n",
    "    # Rolling statistics\n",
    "    for w in [5, 15, 30, 60]:\n",
    "        ws = _scale(w, fps)\n",
    "        roll_params = dict(min_periods=1, center=True)\n",
    "        X[f'cx_m{w}'] = cx.rolling(ws, **roll_params).mean()\n",
    "        X[f'cy_m{w}'] = cy.rolling(ws, **roll_params).mean()\n",
    "        X[f'cx_s{w}'] = cx.rolling(ws, **roll_params).std()\n",
    "        X[f'cy_s{w}'] = cy.rolling(ws, **roll_params).std()\n",
    "        X[f'x_rng{w}'] = cx.rolling(ws, **roll_params).max() - cx.rolling(ws, **roll_params).min()\n",
    "        X[f'y_rng{w}'] = cy.rolling(ws, **roll_params).max() - cy.rolling(ws, **roll_params).min()\n",
    "\n",
    "        # Displacement and activity\n",
    "        X[f'disp{w}'] = np.sqrt(\n",
    "            cx.diff().rolling(ws, min_periods=1).sum()**2 +\n",
    "            cy.diff().rolling(ws, min_periods=1).sum()**2\n",
    "        )\n",
    "        X[f'act{w}'] = np.sqrt(\n",
    "            cx.diff().rolling(ws, min_periods=1).var() +\n",
    "            cy.diff().rolling(ws, min_periods=1).var()\n",
    "        )\n",
    "\n",
    "    # Advanced features\n",
    "    X = add_curvature_features(X, cx, cy, fps)\n",
    "    X = add_multiscale_features(X, cx, cy, fps)\n",
    "    X = add_state_features(X, cx, cy, fps)\n",
    "    X = add_longrange_features(X, cx, cy, fps)\n",
    "\n",
    "    # Distance pairs between ALL body parts (ALWAYS added)\n",
    "    for p1, p2 in itertools.combinations(cfg.KEY_PARTS, 2):\n",
    "        p1x, p1y = get_part(p1)\n",
    "        p2x, p2y = get_part(p2)\n",
    "        X[f\"{p1}+{p2}\"] = np.sqrt((p1x - p2x)**2 + (p1y - p2y)**2)\n",
    "\n",
    "    # Body shape features (ALWAYS added)\n",
    "    nose_x, nose_y = get_part('nose')\n",
    "    tail_x, tail_y = get_part('tail_base')\n",
    "    \n",
    "    v1x = nose_x - cx\n",
    "    v1y = nose_y - cy\n",
    "    v2x = tail_x - cx\n",
    "    v2y = tail_y - cy\n",
    "    X['body_ang'] = (v1x * v2x + v1y * v2y) / (\n",
    "        np.sqrt(v1x**2 + v1y**2) * np.sqrt(v2x**2 + v2y**2) + 1e-6\n",
    "    )\n",
    "\n",
    "    # Elongation (ALWAYS added)\n",
    "    nose_tail_dist = np.sqrt((nose_x - tail_x)**2 + (nose_y - tail_y)**2)\n",
    "    ear_l_x, ear_l_y = get_part('ear_left')\n",
    "    ear_r_x, ear_r_y = get_part('ear_right')\n",
    "    ear_dist = np.sqrt((ear_l_x - ear_r_x)**2 + (ear_l_y - ear_r_y)**2)\n",
    "    X['elong'] = nose_tail_dist / (ear_dist + 1e-6)\n",
    "\n",
    "    # Speed with lags (ALWAYS added)\n",
    "    lag = _scale(10, fps)\n",
    "    X['sp_lf'] = np.sqrt((ear_l_x - ear_l_x.shift(lag))**2 + \n",
    "                         (ear_l_y - ear_l_y.shift(lag))**2)\n",
    "    X['sp_rt'] = np.sqrt((ear_r_x - ear_r_x.shift(lag))**2 + \n",
    "                         (ear_r_y - ear_r_y.shift(lag))**2)\n",
    "    X['sp_lf2'] = np.sqrt((ear_l_x - tail_x.shift(lag))**2 + \n",
    "                          (ear_l_y - tail_y.shift(lag))**2)\n",
    "    X['sp_rt2'] = np.sqrt((ear_r_x - tail_x.shift(lag))**2 + \n",
    "                          (ear_r_y - tail_y.shift(lag))**2)\n",
    "\n",
    "    # Nose-tail features with multiple lags (ALWAYS added)\n",
    "    nt_dist = nose_tail_dist\n",
    "    for lag in [10, 20, 40]:\n",
    "        l = _scale(lag, fps)\n",
    "        X[f'nt_lg{lag}'] = nt_dist.shift(l)\n",
    "        X[f'nt_df{lag}'] = nt_dist - nt_dist.shift(l)\n",
    "\n",
    "    # Ear features (ALWAYS added)\n",
    "    ear_d = ear_dist\n",
    "    for off in [-30, -20, -10, 10, 20, 30]:\n",
    "        o = _scale_signed(off, fps)\n",
    "        X[f'ear_o{off}'] = ear_d.shift(-o)\n",
    "    w = _scale(30, fps)\n",
    "    X['ear_con'] = ear_d.rolling(w, min_periods=1, center=True).std() / \\\n",
    "                   (ear_d.rolling(w, min_periods=1, center=True).mean() + 1e-6)\n",
    "\n",
    "    return X.fillna(0).astype(np.float32)\n",
    "\n",
    "\n",
    "def extract_pair_features(pvid, agent_id, target_id, fps=30.0):\n",
    "    \"\"\"\n",
    "    Extract features for mouse pair interactions\n",
    "    \n",
    "    Includes: ~100-150 pair-specific features\n",
    "    - Cross-mouse distances for all body part pairs\n",
    "    - Relative orientation\n",
    "    - Approach/retreat dynamics\n",
    "    - Distance categories (very close, close, medium, far)\n",
    "    - Distance statistics over multiple windows\n",
    "    - Interaction intensity\n",
    "    - Velocity alignment and coordination\n",
    "    - Leader/follower dynamics\n",
    "    - Chase behavior\n",
    "    - Nose-to-nose features\n",
    "    - Velocity alignment at multiple offsets\n",
    "    \"\"\"\n",
    "    if agent_id not in pvid.columns.get_level_values('mouse_id') or \\\n",
    "       target_id not in pvid.columns.get_level_values('mouse_id'):\n",
    "        return None\n",
    "\n",
    "    agent_data = pvid.loc[:, agent_id]\n",
    "    target_data = pvid.loc[:, target_id]\n",
    "\n",
    "    X = pd.DataFrame(index=pvid.index)\n",
    "\n",
    "    # Inter-mouse distances for all body part pairs\n",
    "    for p1 in cfg.KEY_PARTS:\n",
    "        for p2 in cfg.KEY_PARTS:\n",
    "            if p1 in agent_data.columns and p2 in target_data.columns:\n",
    "                X[f\"12+{p1}+{p2}\"] = np.sqrt(\n",
    "                    (agent_data[p1]['x'] - target_data[p2]['x'])**2 +\n",
    "                    (agent_data[p1]['y'] - target_data[p2]['y'])**2\n",
    "                )\n",
    "\n",
    "    # Relative orientation\n",
    "    if all(p in agent_data.columns for p in ['nose', 'tail_base']) and \\\n",
    "       all(p in target_data.columns for p in ['nose', 'tail_base']):\n",
    "        dir_A = agent_data['nose'] - agent_data['tail_base']\n",
    "        dir_B = target_data['nose'] - target_data['tail_base']\n",
    "        X['rel_ori'] = (dir_A['x'] * dir_B['x'] + dir_A['y'] * dir_B['y']) / (\n",
    "            np.sqrt(dir_A['x']**2 + dir_A['y']**2) *\n",
    "            np.sqrt(dir_B['x']**2 + dir_B['y']**2) + 1e-6\n",
    "        )\n",
    "\n",
    "    # Approach/retreat\n",
    "    if 'nose' in agent_data.columns and 'nose' in target_data.columns:\n",
    "        current_dist = np.sqrt(\n",
    "            (agent_data['nose']['x'] - target_data['nose']['x'])**2 +\n",
    "            (agent_data['nose']['y'] - target_data['nose']['y'])**2\n",
    "        )\n",
    "        lag = _scale(10, fps)\n",
    "        past_dist = current_dist.shift(lag)\n",
    "        X['appr'] = past_dist - current_dist  # Positive = approaching\n",
    "\n",
    "    # Distance statistics and categories\n",
    "    if 'body_center' in agent_data.columns and 'body_center' in target_data.columns:\n",
    "        center_dist = np.sqrt(\n",
    "            (agent_data['body_center']['x'] - target_data['body_center']['x'])**2 +\n",
    "            (agent_data['body_center']['y'] - target_data['body_center']['y'])**2\n",
    "        )\n",
    "\n",
    "        # Distance categories\n",
    "        X['v_cls'] = (center_dist < 5.0).astype(float)\n",
    "        X['cls'] = ((center_dist >= 5.0) & (center_dist < 15.0)).astype(float)\n",
    "        X['med'] = ((center_dist >= 15.0) & (center_dist < 30.0)).astype(float)\n",
    "        X['far'] = (center_dist >= 30.0).astype(float)\n",
    "\n",
    "        # Distance rolling statistics\n",
    "        for w in [5, 15, 30, 60]:\n",
    "            ws = _scale(w, fps)\n",
    "            roll_params = dict(min_periods=1, center=True)\n",
    "            X[f'd_m{w}'] = center_dist.rolling(ws, **roll_params).mean()\n",
    "            X[f'd_s{w}'] = center_dist.rolling(ws, **roll_params).std()\n",
    "            X[f'd_mn{w}'] = center_dist.rolling(ws, **roll_params).min()\n",
    "            X[f'd_mx{w}'] = center_dist.rolling(ws, **roll_params).max()\n",
    "\n",
    "            # Interaction intensity\n",
    "            d_var = center_dist.rolling(ws, **roll_params).var()\n",
    "            X[f'int{w}'] = 1 / (1 + d_var)\n",
    "\n",
    "            # Coordination (velocity dot product)\n",
    "            Axd = agent_data['body_center']['x'].diff()\n",
    "            Ayd = agent_data['body_center']['y'].diff()\n",
    "            Bxd = target_data['body_center']['x'].diff()\n",
    "            Byd = target_data['body_center']['y'].diff()\n",
    "            coord = Axd * Bxd + Ayd * Byd\n",
    "            X[f'co_m{w}'] = coord.rolling(ws, **roll_params).mean()\n",
    "            X[f'co_s{w}'] = coord.rolling(ws, **roll_params).std()\n",
    "\n",
    "        # Add interaction features (chase, leader/follower)\n",
    "        X = add_interaction_features(X, {'A': agent_data, 'B': target_data},\n",
    "                                    agent_data.columns.get_level_values(0).unique(),\n",
    "                                    target_data.columns.get_level_values(0).unique(),\n",
    "                                    fps)\n",
    "\n",
    "    # Nose-to-nose features\n",
    "    if 'nose' in agent_data.columns and 'nose' in target_data.columns:\n",
    "        nn = np.sqrt((agent_data['nose']['x'] - target_data['nose']['x'])**2 +\n",
    "                     (agent_data['nose']['y'] - target_data['nose']['y'])**2)\n",
    "        for lag in [10, 20, 40]:\n",
    "            l = _scale(lag, fps)\n",
    "            X[f'nn_lg{lag}'] = nn.shift(l)\n",
    "            X[f'nn_ch{lag}'] = nn - nn.shift(l)\n",
    "            is_cl = (nn < 10.0).astype(float)\n",
    "            X[f'cl_ps{lag}'] = is_cl.rolling(l, min_periods=1).mean()\n",
    "\n",
    "    # Velocity alignment at multiple offsets\n",
    "    if 'body_center' in agent_data.columns and 'body_center' in target_data.columns:\n",
    "        Avx = agent_data['body_center']['x'].diff()\n",
    "        Avy = agent_data['body_center']['y'].diff()\n",
    "        Bvx = target_data['body_center']['x'].diff()\n",
    "        Bvy = target_data['body_center']['y'].diff()\n",
    "        val = (Avx * Bvx + Avy * Bvy) / (np.sqrt(Avx**2 + Avy**2) * np.sqrt(Bvx**2 + Bvy**2) + 1e-6)\n",
    "\n",
    "        for off in [-30, -20, -10, 0, 10, 20, 30]:\n",
    "            o = _scale_signed(off, fps)\n",
    "            X[f'va_{off}'] = val.shift(-o)\n",
    "\n",
    "        w = _scale(30, fps)\n",
    "        center_dist_sq = (agent_data['body_center']['x'] - target_data['body_center']['x'])**2 + \\\n",
    "                         (agent_data['body_center']['y'] - target_data['body_center']['y'])**2\n",
    "        X['int_con'] = center_dist_sq.rolling(w, min_periods=1, center=True).std() / \\\n",
    "                       (center_dist_sq.rolling(w, min_periods=1, center=True).mean() + 1e-6)\n",
    "\n",
    "    return X.fillna(0).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 1: TRANSFORMER TRAINING\")\n",
    "print(\"=\"*80)\n",
    "t1 = time.time()\n",
    "\n",
    "all_seq, all_lab = [], []\n",
    "sample = train_labeled.sample(n=min(cfg.MAX_TRAIN_VIDEOS, len(train_labeled)), random_state=42)\n",
    "\n",
    "for _, row in tqdm(sample.iterrows(), total=len(sample), desc=\"Loading\"):\n",
    "    if type(row.behaviors_labeled) != str:\n",
    "        continue\n",
    "    try:\n",
    "        vid = pd.read_parquet(f\"{cfg.TRAIN_TRACKING_DIR}/{row.lab_id}/{row.video_id}.parquet\")\n",
    "        if len(vid.bodypart.unique()) > 5:\n",
    "            vid = vid[~vid.bodypart.isin(cfg.DROP_PARTS)]\n",
    "        pvid = vid.pivot(columns=['mouse_id','bodypart'], index='video_frame', values=['x','y'])\n",
    "        pvid = pvid.reorder_levels([1,2,0], axis=1).T.sort_index().T / row.pix_per_cm_approx\n",
    "        \n",
    "        annot = pd.read_parquet(f\"{cfg.TRAIN_ANNOTATION_DIR}/{row.lab_id}/{row.video_id}.parquet\")\n",
    "        \n",
    "        if 1 not in pvid.columns.get_level_values('mouse_id'):\n",
    "            continue\n",
    "        md = pvid.loc[:, 1]\n",
    "        \n",
    "        if len(md) > cfg.MAX_FRAMES:\n",
    "            md = md.iloc[::cfg.STRIDE]\n",
    "        \n",
    "        feat = extract_pose(md, cfg.KEY_PARTS)\n",
    "        lab = np.zeros(len(md))\n",
    "        \n",
    "        for _, a in annot[(annot.agent_id==1) & (annot.target_id==1)].iterrows():\n",
    "            s, e = a.start_frame, a.stop_frame\n",
    "            if len(md) > cfg.MAX_FRAMES:\n",
    "                s, e = s//cfg.STRIDE, e//cfg.STRIDE\n",
    "            if s < len(lab) and e <= len(lab):\n",
    "                lab[s:e] = 1.0\n",
    "        \n",
    "        seqs, tgts = make_seqs(feat, lab, cfg.SEQ_LEN, cfg.SEQ_LEN//2)\n",
    "        if len(seqs) > 0:\n",
    "            all_seq.append(seqs)\n",
    "            all_lab.append(tgts)\n",
    "        del vid, pvid\n",
    "        gc.collect()\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "X = np.concatenate(all_seq)\n",
    "y = np.concatenate(all_lab)\n",
    "print(f\"\\nSequences: {len(X):,}, Pos: {y.mean():.2%}\")\n",
    "\n",
    "ds = SeqData(X, y)\n",
    "loader = DataLoader(ds, batch_size=cfg.BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "\n",
    "pos_ratio = y.mean()\n",
    "del all_seq, all_lab, X, y\n",
    "gc.collect()\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=cfg.LR, weight_decay=0.01)\n",
    "\n",
    "pos_weight = torch.tensor([(1 - pos_ratio) / pos_ratio]).to(device)\n",
    "crit = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "scaler = torch.cuda.amp.GradScaler() if device.type == 'cuda' else None\n",
    "\n",
    "print(f\"Positive class weight: {pos_weight.item():.2f}x\")\n",
    "\n",
    "for ep in range(cfg.TRANSFORMER_EPOCHS):\n",
    "    model.train()\n",
    "    loss_sum = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    pos_pred = 0\n",
    "    pos_true = 0\n",
    "    \n",
    "    for bx, by in tqdm(loader, desc=f\"Epoch {ep+1}\"):\n",
    "        bx, by = bx.to(device), by.to(device)\n",
    "        opt.zero_grad()\n",
    "        if scaler:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                out = model(bx, return_embedding=False)\n",
    "                loss = crit(out, by)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(opt)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(opt)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            out = model(bx, return_embedding=False)\n",
    "            loss = crit(out, by)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            opt.step()\n",
    "        \n",
    "        loss_sum += loss.item()\n",
    "        # Track accuracy and predictions\n",
    "        preds = (torch.sigmoid(out) > 0.5).float()\n",
    "        correct += (preds == by).sum().item()\n",
    "        total += by.size(0)\n",
    "        pos_pred += preds.sum().item()\n",
    "        pos_true += by.sum().item()\n",
    "    \n",
    "    acc = correct / total\n",
    "    pred_rate = pos_pred / total\n",
    "    true_rate = pos_true / total\n",
    "    print(f\"Epoch {ep+1} Loss: {loss_sum/len(loader):.4f}, Acc: {acc:.4f}, \"\n",
    "          f\"PredPos: {pred_rate:.2%}, TruePos: {true_rate:.2%}\")\n",
    "\n",
    "torch.save({\n",
    "    'model': model.state_dict(),\n",
    "    'd_model': model.d_model,\n",
    "    'model_type': 'TSTransformerEncoder'\n",
    "}, cfg.TRANSFORMER_PATH)\n",
    "model.eval()\n",
    "print(f\"\\nPhase 1: {(time.time()-t1)/60:.1f}min\")\n",
    "del loader, ds\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2: Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 2: FEATURE EXTRACTION (Enhanced with FPS-Aware Features)\")\n",
    "print(\"=\"*80)\n",
    "t2 = time.time()\n",
    "\n",
    "# Load transformer\n",
    "ckpt = torch.load(cfg.TRANSFORMER_PATH)\n",
    "model = TSTransformerEncoder(\n",
    "    n_features=cfg.N_FEATURES,\n",
    "    d_model=ckpt['d_model'],\n",
    "    n_heads=8,\n",
    "    n_layers=3,\n",
    "    dim_feedforward=512,\n",
    "    dropout=0.1,\n",
    "    max_len=cfg.SEQ_LEN * 2\n",
    ").to(device)\n",
    "model.load_state_dict(ckpt['model'])\n",
    "model.eval()\n",
    "\n",
    "# Build FPS lookup from metadata\n",
    "fps_lookup = {}\n",
    "for _, row in train_labeled.iterrows():\n",
    "    if 'frames_per_second' in train_labeled.columns and pd.notnull(row.get('frames_per_second')):\n",
    "        fps_lookup[row.video_id] = float(row['frames_per_second'])\n",
    "\n",
    "print(f\"FPS lookup: {len(fps_lookup)} videos with FPS metadata (default: {cfg.FPS_DEFAULT} FPS)\")\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_emb(data, model, slen):\n",
    "    feat = extract_pose(data, cfg.KEY_PARTS)\n",
    "    if len(feat) < slen:\n",
    "        return None\n",
    "    seqs = [feat[i:i+slen] for i in range(0, len(feat)-slen+1, slen//2)]\n",
    "    seqs_t = torch.FloatTensor(seqs).to(device)\n",
    "    embs = []\n",
    "    for i in range(0, len(seqs_t), 64):\n",
    "        embs.append(model(seqs_t[i:i+64], return_embedding=True).cpu().numpy())\n",
    "    embs = np.concatenate(embs)\n",
    "    frame_emb = np.zeros((len(feat), embs.shape[1]))\n",
    "    for i, e in enumerate(embs):\n",
    "        s, end = i*(slen//2), i*(slen//2)+slen\n",
    "        frame_emb[s:end] = e\n",
    "    return frame_emb\n",
    "\n",
    "def proc_video(row, mode='train', expected_dim=None, fps_lookup=None):\n",
    "    tdir = cfg.TRAIN_TRACKING_DIR if mode == 'train' else cfg.TEST_TRACKING_DIR\n",
    "    try:\n",
    "        vid = pd.read_parquet(f\"{tdir}/{row.lab_id}/{row.video_id}.parquet\")\n",
    "        if len(vid.bodypart.unique()) > 5:\n",
    "            vid = vid[~vid.bodypart.isin(cfg.DROP_PARTS)]\n",
    "        pvid = vid.pivot(columns=['mouse_id','bodypart'], index='video_frame', values=['x','y'])\n",
    "        pvid = pvid.reorder_levels([1,2,0], axis=1).T.sort_index().T / row.pix_per_cm_approx\n",
    "        \n",
    "        fps = cfg.FPS_DEFAULT\n",
    "        if fps_lookup and row.video_id in fps_lookup:\n",
    "            fps = fps_lookup[row.video_id]\n",
    "        \n",
    "        if 1 not in pvid.columns.get_level_values('mouse_id'):\n",
    "            return None\n",
    "        md = pvid.loc[:, 1]\n",
    "        \n",
    "        downsampled = len(md) > cfg.MAX_FRAMES\n",
    "        if downsampled:\n",
    "            md = md.iloc[::cfg.STRIDE].reset_index(drop=True)\n",
    "        \n",
    "        emb = get_emb(md, model, cfg.SEQ_LEN)\n",
    "        if emb is None:\n",
    "            return None\n",
    "        \n",
    "        hand = enhanced_hand_feat(md, fps=fps)\n",
    "        combined = np.concatenate([emb, hand.values], axis=1)\n",
    "        \n",
    "        if expected_dim is not None:\n",
    "            if combined.shape[1] < expected_dim:\n",
    "                padding = np.zeros((combined.shape[0], expected_dim - combined.shape[1]))\n",
    "                combined = np.concatenate([combined, padding], axis=1)\n",
    "            elif combined.shape[1] > expected_dim:\n",
    "                combined = combined[:, :expected_dim]\n",
    "        \n",
    "        if mode == 'train':\n",
    "            annot = pd.read_parquet(f\"{cfg.TRAIN_ANNOTATION_DIR}/{row.lab_id}/{row.video_id}.parquet\")\n",
    "            behav = json.loads(row.behaviors_labeled)\n",
    "            behav = [b.replace(\"'\",\"\").split(',') for b in behav if '1,self' in b or '1,1' in b]\n",
    "            actions = [b[2] for b in behav if b[0]=='mouse1' and b[1] in ['self','mouse1']]\n",
    "            \n",
    "            if len(actions) == 0:\n",
    "                return None\n",
    "            \n",
    "            labels = pd.DataFrame(0.0, columns=actions, index=range(len(md)))\n",
    "            \n",
    "            for _, a in annot[(annot.agent_id==1) & (annot.target_id==1)].iterrows():\n",
    "                if a.action in actions:\n",
    "                    s, e = a.start_frame, a.stop_frame\n",
    "                    \n",
    "                    if downsampled:\n",
    "                        s, e = s // cfg.STRIDE, e // cfg.STRIDE\n",
    "                    \n",
    "                    s = max(0, min(s, len(labels)))\n",
    "                    e = max(0, min(e, len(labels)))\n",
    "                    \n",
    "                    if s < e and s < len(labels):\n",
    "                        labels.loc[s:e-1, a.action] = 1.0\n",
    "            \n",
    "            return {'X': combined, 'y': labels, 'vid': row.video_id}\n",
    "        return {'X': combined, 'vid': row.video_id}\n",
    "    except Exception as ex:\n",
    "        return None\n",
    "\n",
    "print(\"Extracting training features with FPS-aware feature engineering...\")\n",
    "train_data = []\n",
    "total_pos_count = 0\n",
    "videos_with_labels = 0\n",
    "feature_dims_list = []\n",
    "\n",
    "for _, row in tqdm(train_labeled.iterrows(), total=len(train_labeled)):\n",
    "    if type(row.behaviors_labeled) != str:\n",
    "        continue\n",
    "    res = proc_video(row, 'train', fps_lookup=fps_lookup)\n",
    "    if res:\n",
    "        train_data.append(res)\n",
    "        feature_dims_list.append(res['X'].shape[1])\n",
    "        pos_in_video = res['y'].sum().sum()\n",
    "        if pos_in_video > 0:\n",
    "            videos_with_labels += 1\n",
    "        total_pos_count += pos_in_video\n",
    "\n",
    "with open(cfg.FEATURES_PATH, 'wb') as f:\n",
    "    pickle.dump(train_data, f)\n",
    "\n",
    "if len(feature_dims_list) > 0:\n",
    "    feature_dim = max(set(feature_dims_list), key=feature_dims_list.count)\n",
    "    print(f\"\\nFeature dimension: {feature_dim} (256 transformer + ~{feature_dim-256} handcrafted)\")\n",
    "print(f\"Features saved: {len(train_data)} videos\")\n",
    "print(f\"Videos with positive labels: {videos_with_labels}/{len(train_data)}\")\n",
    "print(f\"Total positive labels: {int(total_pos_count)}\")\n",
    "print(f\"Phase 2: {(time.time()-t2)/60:.1f}min\")\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 3: XGBOOST TRAINING\")\n",
    "print(\"=\"*80)\n",
    "t3 = time.time()\n",
    "\n",
    "with open(cfg.FEATURES_PATH, 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "all_actions = set()\n",
    "for d in data:\n",
    "    all_actions.update(d['y'].columns)\n",
    "print(f\"Actions: {sorted(all_actions)}\")\n",
    "\n",
    "feature_dims = [d['X'].shape[1] for d in data]\n",
    "unique_dims = set(feature_dims)\n",
    "if len(unique_dims) > 1:\n",
    "    print(f\"ERROR: Inconsistent feature dimensions found: {unique_dims}\")\n",
    "    print(f\"Feature dimension counts: {[(dim, feature_dims.count(dim)) for dim in unique_dims]}\")\n",
    "    raise ValueError(\"Feature dimensions are inconsistent! Check enhanced_hand_feat() function.\")\n",
    "\n",
    "expected_dim = feature_dims[0]\n",
    "print(f\"Feature dimension: {expected_dim} (256 transformer + {expected_dim-256} handcrafted)\")\n",
    "\n",
    "models = {}\n",
    "thresholds = {}\n",
    "\n",
    "for action in sorted(all_actions):\n",
    "    Xs, ys, vids = [], [], []\n",
    "    for d in data:\n",
    "        if action in d['y'].columns:\n",
    "            Xs.append(d['X'])\n",
    "            ys.append(d['y'][action].values)\n",
    "            vids.extend([d['vid']]*len(d['X']))\n",
    "    \n",
    "    if len(Xs) == 0:\n",
    "        continue\n",
    "    \n",
    "    X = np.vstack(Xs)\n",
    "    y = np.concatenate(ys)\n",
    "    vids = np.array(vids)\n",
    "    \n",
    "    if np.any(np.isnan(X)) or np.any(np.isinf(X)):\n",
    "        print(f\"{action}: WARNING - NaN/Inf detected in features, replacing with 0\")\n",
    "        X = np.nan_to_num(X, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    \n",
    "    pos_count = y.sum()\n",
    "    pos_rate = y.mean()\n",
    "    \n",
    "    if pos_count < 10:\n",
    "        print(f\"{action}: Skip (only {int(pos_count)} positives)\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"{action}: {len(X):,} samples, {int(pos_count)} pos ({pos_rate:.2%})\")\n",
    "    \n",
    "    cv = StratifiedGroupKFold(n_splits=cfg.N_SPLITS)\n",
    "    oof = np.zeros(len(X))\n",
    "    folds = []\n",
    "    \n",
    "    xgb_params = cfg.XGB_PARAMS.copy()\n",
    "    xgb_params['scale_pos_weight'] = (1 - pos_rate) / pos_rate\n",
    "    \n",
    "    for fold_idx, (train_i, val_i) in enumerate(cv.split(X, y, vids)):\n",
    "        m = XGBClassifier(**xgb_params)\n",
    "        m.fit(X[train_i], y[train_i])\n",
    "        oof[val_i] = m.predict_proba(X[val_i])[:, 1]\n",
    "        folds.append(m)\n",
    "    \n",
    "    oof_min, oof_max, oof_mean = oof.min(), oof.max(), oof.mean()\n",
    "    oof_unique = len(np.unique(oof))\n",
    "    \n",
    "    if oof_unique < 10:\n",
    "        print(f\"  WARNING: Only {oof_unique} unique OOF predictions!\")\n",
    "    \n",
    "    if oof_max - oof_min < 0.01:\n",
    "        print(f\"  WARNING: OOF predictions have very low variance [{oof_min:.4f}, {oof_max:.4f}]\")\n",
    "    \n",
    "    best_thr, best_f1 = 0.5, 0\n",
    "    for thr in np.arange(0.05, 0.95, 0.01):\n",
    "        preds = (oof >= thr).astype(int)\n",
    "        f1 = f1_score(y, preds, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_thr = f1, thr\n",
    "    \n",
    "    models[action] = folds\n",
    "    thresholds[action] = best_thr\n",
    "    \n",
    "    final_preds = (oof >= best_thr).astype(int)\n",
    "    tp = ((final_preds == 1) & (y == 1)).sum()\n",
    "    fp = ((final_preds == 1) & (y == 0)).sum()\n",
    "    fn = ((final_preds == 0) & (y == 1)).sum()\n",
    "    \n",
    "    print(f\"  F1={best_f1:.4f}, thr={best_thr:.3f}, OOF=[{oof_min:.3f},{oof_max:.3f}], \"\n",
    "          f\"TP={tp}, FP={fp}, FN={fn}\")\n",
    "\n",
    "import pickle as pkl\n",
    "with open('models.pkl', 'wb') as f:\n",
    "    pkl.dump({'models': models, 'thresholds': thresholds, 'expected_dim': expected_dim}, f)\n",
    "\n",
    "print(f\"\\nPhase 3: {(time.time()-t3)/60:.1f}min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4: Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PHASE 4: INFERENCE\")\n",
    "print(\"=\"*80)\n",
    "t4 = time.time()\n",
    "\n",
    "# Load transformer\n",
    "ckpt = torch.load(cfg.TRANSFORMER_PATH)\n",
    "model = TSTransformerEncoder(\n",
    "    n_features=cfg.N_FEATURES,\n",
    "    d_model=ckpt['d_model'],\n",
    "    n_heads=8,\n",
    "    n_layers=3,\n",
    "    dim_feedforward=512,\n",
    "    dropout=0.1,\n",
    "    max_len=cfg.SEQ_LEN * 2\n",
    ").to(device)\n",
    "model.load_state_dict(ckpt['model'])\n",
    "model.eval()\n",
    "\n",
    "import pickle as pkl\n",
    "with open('models.pkl', 'rb') as f:\n",
    "    saved = pkl.load(f)\n",
    "    models = saved['models']\n",
    "    thresholds = saved['thresholds']\n",
    "    expected_dim = saved['expected_dim']\n",
    "\n",
    "print(f\"Loaded models for {len(models)} actions, expected feature dim: {expected_dim}\")\n",
    "\n",
    "submissions = []\n",
    "\n",
    "for _, row in tqdm(test.iterrows(), total=len(test), desc=\"Test\"):\n",
    "    if type(row.behaviors_labeled) != str:\n",
    "        continue\n",
    "    \n",
    "    behav = json.loads(row.behaviors_labeled)\n",
    "    behav_parsed = [b.replace(\"'\",\"\").split(',') for b in behav]\n",
    "    \n",
    "    required_behaviors = []\n",
    "    for b in behav_parsed:\n",
    "        if len(b) >= 3:\n",
    "            agent = b[0]\n",
    "            target = b[1]\n",
    "            action = b[2]\n",
    "            required_behaviors.append((agent, target, action))\n",
    "    \n",
    "    res = proc_video(row, 'test', expected_dim=expected_dim)\n",
    "    \n",
    "    if res is None:\n",
    "        for agent_id, target_id, action in required_behaviors:\n",
    "            submissions.append({\n",
    "                'video_id': row.video_id,\n",
    "                'agent_id': agent_id,\n",
    "                'target_id': target_id,\n",
    "                'action': action,\n",
    "                'start_frame': 0,\n",
    "                'stop_frame': 1\n",
    "            })\n",
    "        continue\n",
    "    \n",
    "    X_test = res['X']\n",
    "    n_frames = len(X_test)\n",
    "    \n",
    "    agent_target_groups = {}\n",
    "    for agent_id, target_id, action in required_behaviors:\n",
    "        key = (agent_id, target_id)\n",
    "        if key not in agent_target_groups:\n",
    "            agent_target_groups[key] = []\n",
    "        agent_target_groups[key].append(action)\n",
    "    \n",
    "    for (agent_id, target_id), actions in agent_target_groups.items():\n",
    "        action_probs = {}\n",
    "        \n",
    "        for action in actions:\n",
    "            if action in models:\n",
    "                preds = [m.predict_proba(X_test)[:, 1] for m in models[action]]\n",
    "                prob = np.mean(preds, axis=0)\n",
    "                action_probs[action] = prob\n",
    "            else:\n",
    "                action_probs[action] = np.zeros(n_frames)\n",
    "        \n",
    "        frame_actions = []\n",
    "        for i in range(n_frames):\n",
    "            best_action = None\n",
    "            best_prob = 0.0\n",
    "            \n",
    "            for action in actions:\n",
    "                if action in action_probs:\n",
    "                    prob = action_probs[action][i]\n",
    "                    if action in thresholds and prob >= thresholds[action]:\n",
    "                        if prob > best_prob:\n",
    "                            best_prob = prob\n",
    "                            best_action = action\n",
    "            \n",
    "            frame_actions.append(best_action)\n",
    "        \n",
    "        if len(frame_actions) > 0:\n",
    "            current_action = frame_actions[0]\n",
    "            start_frame = 0\n",
    "            \n",
    "            for i in range(1, len(frame_actions)):\n",
    "                if frame_actions[i] != current_action:\n",
    "                    if current_action is not None:\n",
    "                        orig_start = start_frame * cfg.STRIDE if n_frames < 6000 else start_frame\n",
    "                        orig_stop = i * cfg.STRIDE if n_frames < 6000 else i\n",
    "                        \n",
    "                        submissions.append({\n",
    "                            'video_id': row.video_id,\n",
    "                            'agent_id': agent_id,\n",
    "                            'target_id': target_id,\n",
    "                            'action': current_action,\n",
    "                            'start_frame': orig_start,\n",
    "                            'stop_frame': orig_stop\n",
    "                        })\n",
    "                    \n",
    "                    current_action = frame_actions[i]\n",
    "                    start_frame = i\n",
    "            \n",
    "            if current_action is not None:\n",
    "                orig_start = start_frame * cfg.STRIDE if n_frames < 6000 else start_frame\n",
    "                orig_stop = n_frames * cfg.STRIDE if n_frames < 6000 else n_frames\n",
    "                \n",
    "                submissions.append({\n",
    "                    'video_id': row.video_id,\n",
    "                    'agent_id': agent_id,\n",
    "                    'target_id': target_id,\n",
    "                    'action': current_action,\n",
    "                    'start_frame': orig_start,\n",
    "                    'stop_frame': orig_stop\n",
    "                })\n",
    "\n",
    "all_required = set()\n",
    "for _, row in test.iterrows():\n",
    "    if type(row.behaviors_labeled) != str:\n",
    "        continue\n",
    "    behav = json.loads(row.behaviors_labeled)\n",
    "    behav_parsed = [b.replace(\"'\",\"\").split(',') for b in behav]\n",
    "    for b in behav_parsed:\n",
    "        if len(b) >= 3:\n",
    "            all_required.add((row.video_id, b[0], b[1], b[2]))\n",
    "\n",
    "submitted = set()\n",
    "for sub in submissions:\n",
    "    submitted.add((sub['video_id'], sub['agent_id'], sub['target_id'], sub['action']))\n",
    "\n",
    "for vid, agent, target, action in all_required:\n",
    "    if (vid, agent, target, action) not in submitted:\n",
    "        submissions.append({\n",
    "            'video_id': vid,\n",
    "            'agent_id': agent,\n",
    "            'target_id': target,\n",
    "            'action': action,\n",
    "            'start_frame': 0,\n",
    "            'stop_frame': 1\n",
    "        })\n",
    "\n",
    "if len(submissions) > 0:\n",
    "    sub = pd.DataFrame(submissions)\n",
    "else:\n",
    "    sub = pd.DataFrame([{\n",
    "        'video_id': test.iloc[0].video_id,\n",
    "        'agent_id': 'mouse1',\n",
    "        'target_id': 'mouse2',\n",
    "        'action': 'sniff',\n",
    "        'start_frame': 0,\n",
    "        'stop_frame': 1\n",
    "    }])\n",
    "\n",
    "sub = sub.sort_values(['video_id', 'agent_id', 'target_id', 'action', 'start_frame']).reset_index(drop=True)\n",
    "sub.index.name = 'row_id'\n",
    "sub.to_csv('submission.csv')\n",
    "\n",
    "print(f\"\\nSubmission: {len(sub)} rows\")\n",
    "print(f\"Unique videos: {sub.video_id.nunique()}\")\n",
    "print(f\"Unique behaviors: {len(sub.groupby(['agent_id', 'target_id', 'action']))}\")\n",
    "print(f\"Phase 4: {(time.time()-t4)/60:.1f}min\")\n",
    "print(f\"\\nTOTAL TIME: {(time.time()-t1)/60:.1f}min\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
